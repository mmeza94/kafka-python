{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "\n",
    "def create_topics(list_topics):\n",
    "    broker = \"host.docker.internal:9092\"\n",
    "    admin = KafkaAdminClient(bootstrap_servers=broker)\n",
    "    topics = [NewTopic(name=topic,num_partitions=1,replication_factor=1) for topic in list_topics]\n",
    "    admin.create_topics(topics)\n",
    "\n",
    "create_topics([\"tpc-test-create\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consultando la descripción de los tópicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'error_code': 0,\n",
       "  'topic': 'tpc-test',\n",
       "  'is_internal': False,\n",
       "  'partitions': [{'error_code': 0,\n",
       "    'partition': 0,\n",
       "    'leader': 0,\n",
       "    'replicas': [0],\n",
       "    'isr': [0],\n",
       "    'offline_replicas': []}]}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "from kafka.errors import UnknownTopicOrPartitionError\n",
    "\n",
    "\n",
    "def describe_topics():\n",
    "    broker = \"host.docker.internal:9092\"\n",
    "    admin = KafkaAdminClient(bootstrap_servers=broker)\n",
    "    topics_descriptions = admin.describe_topics(topics=[\"tpc-test\"])\n",
    "    return topics_descriptions\n",
    "\n",
    "td = describe_topics()\n",
    "td\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminando tópicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "from kafka.errors import UnknownTopicOrPartitionError\n",
    "\n",
    "def delete_topics(list_topics:list):\n",
    "    try:\n",
    "        broker = \"host.docker.internal:9092\"\n",
    "        admin = KafkaAdminClient(bootstrap_servers=broker)\n",
    "        admin.delete_topics(list_topics)\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "\n",
    "delete_topics([\"tpc-test-create\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describiendo resource configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   El método describe_configs devuelve una lista de DescribeConfigsResponse_v2\n",
    "-   Podemos acceder a los recursos con la propiedad resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0,\n",
       "   '',\n",
       "   2,\n",
       "   'tpc-test',\n",
       "   [('compression.type', 'producer', False, 5, False, []),\n",
       "    ('confluent.tier.cleaner.compact.min.efficiency',\n",
       "     '0.5',\n",
       "     False,\n",
       "     5,\n",
       "     False,\n",
       "     []),\n",
       "    ('confluent.value.schema.validation', 'false', False, 5, False, []),\n",
       "    ('leader.replication.throttled.replicas', '', False, 5, False, []),\n",
       "    ('confluent.stray.log.max.deletions.per.run', '72', False, 5, False, []),\n",
       "    ('confluent.key.subject.name.strategy',\n",
       "     'io.confluent.kafka.serializers.subject.TopicNameStrategy',\n",
       "     False,\n",
       "     5,\n",
       "     False,\n",
       "     []),\n",
       "    ('message.downconversion.enable', 'true', False, 5, False, []),\n",
       "    ('min.insync.replicas', '1', False, 5, False, []),\n",
       "    ('segment.jitter.ms', '0', False, 5, False, []),\n",
       "    ('confluent.stray.log.delete.delay.ms', '604800000', False, 5, False, []),\n",
       "    ('confluent.tier.cleaner.enable', 'false', False, 5, False, []),\n",
       "    ('confluent.compacted.topic.prefer.tier.fetch.ms',\n",
       "     '-1',\n",
       "     False,\n",
       "     5,\n",
       "     False,\n",
       "     []),\n",
       "    ('cleanup.policy', 'delete', False, 5, False, []),\n",
       "    ('flush.ms', '9223372036854775807', False, 5, False, []),\n",
       "    ('confluent.tier.local.hotset.ms', '86400000', False, 5, False, []),\n",
       "    ('follower.replication.throttled.replicas', '', False, 5, False, []),\n",
       "    ('confluent.tier.local.hotset.bytes', '-1', False, 5, False, []),\n",
       "    ('confluent.value.subject.name.strategy',\n",
       "     'io.confluent.kafka.serializers.subject.TopicNameStrategy',\n",
       "     False,\n",
       "     5,\n",
       "     False,\n",
       "     []),\n",
       "    ('segment.bytes', '1073741824', False, 5, False, []),\n",
       "    ('retention.ms', '604800000', False, 5, False, []),\n",
       "    ('flush.messages', '9223372036854775807', False, 5, False, []),\n",
       "    ('confluent.tier.enable', 'false', False, 5, False, []),\n",
       "    ('confluent.tier.segment.hotset.roll.min.bytes',\n",
       "     '104857600',\n",
       "     False,\n",
       "     5,\n",
       "     False,\n",
       "     []),\n",
       "    ('confluent.segment.speculative.prefetch.enable',\n",
       "     'false',\n",
       "     False,\n",
       "     5,\n",
       "     False,\n",
       "     []),\n",
       "    ('message.format.version', '3.0-IV1', False, 5, False, []),\n",
       "    ('confluent.min.segment.ms', '1', False, 5, False, []),\n",
       "    ('max.compaction.lag.ms', '9223372036854775807', False, 5, False, []),\n",
       "    ('confluent.tier.cleaner.compact.segment.min.bytes',\n",
       "     '20971520',\n",
       "     False,\n",
       "     5,\n",
       "     False,\n",
       "     []),\n",
       "    ('file.delete.delay.ms', '60000', False, 5, False, []),\n",
       "    ('confluent.cluster.link.allow.legacy.message.format',\n",
       "     'true',\n",
       "     False,\n",
       "     5,\n",
       "     False,\n",
       "     []),\n",
       "    ('max.message.bytes', '1048588', False, 5, False, []),\n",
       "    ('confluent.tier.cleaner.dual.compaction', 'false', False, 5, False, []),\n",
       "    ('min.compaction.lag.ms', '0', False, 5, False, []),\n",
       "    ('message.timestamp.type', 'CreateTime', False, 5, False, []),\n",
       "    ('preallocate', 'false', False, 5, False, []),\n",
       "    ('confluent.placement.constraints', '', False, 5, False, []),\n",
       "    ('min.cleanable.dirty.ratio', '0.5', False, 5, False, []),\n",
       "    ('index.interval.bytes', '4096', False, 5, False, []),\n",
       "    ('unclean.leader.election.enable', 'false', False, 5, False, []),\n",
       "    ('retention.bytes', '-1', False, 5, False, []),\n",
       "    ('delete.retention.ms', '86400000', False, 5, False, []),\n",
       "    ('confluent.tier.cleaner.min.cleanable.ratio',\n",
       "     '0.75',\n",
       "     False,\n",
       "     5,\n",
       "     False,\n",
       "     []),\n",
       "    ('confluent.prefer.tier.fetch.ms', '-1', False, 5, False, []),\n",
       "    ('confluent.key.schema.validation', 'false', False, 5, False, []),\n",
       "    ('segment.ms', '604800000', False, 5, False, []),\n",
       "    ('message.timestamp.difference.max.ms',\n",
       "     '9223372036854775807',\n",
       "     False,\n",
       "     5,\n",
       "     False,\n",
       "     []),\n",
       "    ('segment.index.bytes', '10485760', False, 5, False, [])])]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kafka.admin import KafkaAdminClient, ConfigResource\n",
    "from kafka.admin.config_resource import ConfigResourceType\n",
    "\n",
    "TOPIC_NAME = \"tpc-test\"\n",
    "\n",
    "def describe_configs():\n",
    "    broker = \"host.docker.internal:9092\"\n",
    "    admin = KafkaAdminClient(bootstrap_servers=broker)\n",
    "    # 1. Define el recurso de configuración para el tópico\n",
    "    config_resource = ConfigResource(ConfigResourceType.TOPIC, TOPIC_NAME)\n",
    "    #Siempre que se buscar describir recursos se debe de pasarla una lista de instancias de ConfigResource\n",
    "    configs = admin.describe_configs([config_resource])\n",
    "    config_resources = [c.resources for c in configs]\n",
    "    return config_resources\n",
    "\n",
    "resources = describe_configs()\n",
    "resources \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring Consumer Groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listando todos los grupos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('test_3', 'consumer')\n",
      "('test_4', 'consumer')\n"
     ]
    }
   ],
   "source": [
    "from kafka.admin import KafkaAdminClient\n",
    "\n",
    "def list_consumer_groups():\n",
    "    broker = \"host.docker.internal:9092\"\n",
    "    admin = KafkaAdminClient(bootstrap_servers=broker)\n",
    "    consumer_groups = admin.list_consumer_groups()\n",
    "    return consumer_groups\n",
    "\n",
    "groups = list_consumer_groups()\n",
    "\n",
    "for g in groups:\n",
    "    print(g)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Consultando la información de un grupo en específico\n",
    "-   Retorna una lista de instancias de GroupInformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroupInformation(error_code=0, group='test_2', state='Dead', protocol_type='', protocol='', members=[], authorized_operations=None)\n"
     ]
    }
   ],
   "source": [
    "from kafka.admin import KafkaAdminClient\n",
    "\n",
    "def describe_consumer_group(consumer_group):\n",
    "    broker = \"host.docker.internal:9092\"\n",
    "    admin = KafkaAdminClient(bootstrap_servers=broker)\n",
    "    consumer_groups = admin.describe_consumer_groups([consumer_group])\n",
    "    return consumer_groups\n",
    "\n",
    "groups = describe_consumer_group(\"test_2\")\n",
    "\n",
    "for g in groups:\n",
    "    print(g)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listando el ultimo offset de un consumer_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{TopicPartition(topic='tpc-test', partition=0): OffsetAndMetadata(offset=3, metadata='')}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broker = \"host.docker.internal:9092\"\n",
    "admin = KafkaAdminClient(bootstrap_servers=broker)\n",
    "admin.list_consumer_group_offsets(\"test_3\")\n",
    "admin.list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creando mas particiones para un topico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CreatePartitionsResponse_v1(throttle_time_ms=0, topic_errors=[(topic='tpc-test', error_code=0, error_message=None)])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kafka.admin import KafkaAdminClient, NewPartitions\n",
    "\n",
    "TOPIC_NAME = \"tpc-test\"\n",
    "NUM_PARTITIONS = 5  # Asume que el número de particiones actual es 5. Cambia este valor según tu caso.\n",
    "\n",
    "admin = KafkaAdminClient(bootstrap_servers=\"host.docker.internal:9092\")\n",
    "\n",
    "new_partitions = {TOPIC_NAME: NewPartitions(total_count=NUM_PARTITIONS + 2)}\n",
    "\n",
    "admin.create_partitions(new_partitions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaAdminClient, TopicPartition\n",
    "from kafka.admin import DeleteRecordsOptions\n",
    "\n",
    "admin = KafkaAdminClient(bootstrap_servers=\"your_bootstrap_servers\")\n",
    "\n",
    "# Aquí asumo que 'requestOlderOffsets' es un diccionario donde las claves son \n",
    "# instancias de TopicPartition y los valores son los offsets deseados.\n",
    "\n",
    "older_offsets = admin.list_offsets(requestOlderOffsets)\n",
    "\n",
    "records_to_delete = {tp: (0, offset) for tp, offset in older_offsets.items()}\n",
    "\n",
    "options = DeleteRecordsOptions(timeout_ms=10000)  # Ajusta el timeout según lo necesario\n",
    "admin.delete_records(records_to_delete, options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
