{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATE TOPIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "\n",
    "def create_kafka_topic(topic_name, num_partitions, replication_factor, bootstrap_servers):\n",
    "    admin_client = KafkaAdminClient(bootstrap_servers=bootstrap_servers)\n",
    "    topic = NewTopic(name=topic_name,num_partitions=num_partitions,replication_factor=replication_factor)\n",
    "    request = admin_client.create_topics([topic])\n",
    "    print(request)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CreateTopicsResponse_v3(throttle_time_ms=0, topic_errors=[(topic='tpc-test', error_code=0, error_message=None)])\n",
      "Tópico tpc-test creado con éxito.\n"
     ]
    }
   ],
   "source": [
    "# Parámetros del tópico a crear.\n",
    "TOPIC_NAME = \"tpc-test\"\n",
    "NUM_PARTITIONS = 1\n",
    "REPLICATION_FACTOR = 1\n",
    "BOOTSTRAP_SERVERS = \"host.docker.internal:9092\"\n",
    "\n",
    "\n",
    "create_kafka_topic(TOPIC_NAME, NUM_PARTITIONS, REPLICATION_FACTOR, BOOTSTRAP_SERVERS)\n",
    "print(f\"Tópico {TOPIC_NAME} creado con éxito.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminando un tópico de kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeleteTopicsResponse_v3(throttle_time_ms=0, topic_error_codes=[(topic='my_new_topic', error_code=0)])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "admin_client = KafkaAdminClient(bootstrap_servers=BOOTSTRAP_SERVERS)\n",
    "admin_client.delete_topics([TOPIC_NAME])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enviar mensajes a un tópico de kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecordMetadata(topic='my_new_topic', partition=0, topic_partition=TopicPartition(topic='my_new_topic', partition=0), offset=1, timestamp=1697852056496, log_start_offset=0, checksum=None, serialized_key_size=-1, serialized_value_size=42, serialized_header_size=-1)\n"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaProducer\n",
    "import json\n",
    "\n",
    "broker = \"host.docker.internal:9092\"\n",
    "topic = \"my_new_topic\"\n",
    "message = {\n",
    "    \"productId\":1,\n",
    "    \"product_name\":\"IPhone\"\n",
    "}\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=broker,\n",
    "    value_serializer=lambda x:json.dumps(x).encode(\"utf-8\") # para que los mensajes primero los transforme a json y luego los serialice\n",
    ")\n",
    "\n",
    "#res es una promesa, pues el envio es asíncrono, es de tipo FutureRecordMetadata\n",
    "res = producer.send(\n",
    "        topic=topic,\n",
    "        value=message,   \n",
    "    )\n",
    "\n",
    "# para obtener los valores de la respuesta hay que esperarla\n",
    "metadata = res.get(timeout=5)\n",
    "\n",
    "#una vez que se le espera ya te devuelve un objeto de tipo RecordMetdata con información como: topic,topic_partition,offset,etc...\n",
    "print(metadata)\n",
    "\n",
    "# asegura que todos los mensajes que están actualmente en buffer o en tránsito se envíen al broker de Kafka y se confirmen antes de continuar con la ejecución del programa\n",
    "#Es una operación bloqueante, lo que significa que detendrá la ejecución de tu programa hasta que todos los mensajes hayan sido enviados y confirmados por el broker.\n",
    "producer.flush()\n",
    "\n",
    "producer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### creando una funcion para enviar mensajes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "def send_kafka_message(message,topic,config):\n",
    "    producer = KafkaProducer(**config)\n",
    "    response = producer.send(value=message,topic=topic)\n",
    "    response = response.get(timeout=5)\n",
    "    print(response)\n",
    "    producer.flush()\n",
    "    producer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecordMetadata(topic='my_new_topic', partition=0, topic_partition=TopicPartition(topic='my_new_topic', partition=0), offset=2, timestamp=1697852567092, log_start_offset=0, checksum=None, serialized_key_size=-1, serialized_value_size=40, serialized_header_size=-1)\n"
     ]
    }
   ],
   "source": [
    "producer_config = {\n",
    "    \"bootstrap_servers\":\"host.docker.internal:9092\",\n",
    "    \"value_serializer\":lambda x : json.dumps(x).encode(\"utf8\")\n",
    "}\n",
    "\n",
    "message = {\n",
    "    \"productId\":2,\n",
    "    \"product_name\":\"IPad\"\n",
    "}\n",
    "\n",
    "topic = \"my_new_topic\"\n",
    "\n",
    "send_kafka_message(message,topic,producer_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enviando mensajes de forma asíncrona con callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecordMetadata(topic='tpc-test', partition=0, topic_partition=TopicPartition(topic='tpc-test', partition=0), offset=2, timestamp=1698112391140, log_start_offset=0, checksum=None, serialized_key_size=-1, serialized_value_size=39, serialized_header_size=-1)\n"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaProducer\n",
    "import json\n",
    "\n",
    "def on_success(metadata):\n",
    "    print(metadata)\n",
    "\n",
    "def on_failure(excepcion):\n",
    "    print(excepcion)\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers = \"host.docker.internal:9092\",\n",
    "    value_serializer=lambda x:json.dumps(x).encode(\"utf-8\")\n",
    ")\n",
    "\n",
    "\n",
    "message = {\n",
    "    \"productId\": 3,\n",
    "    \"product_name\": \"HIJ\"\n",
    "}\n",
    "\n",
    "topic = \"tpc-test\"\n",
    "\n",
    "producer.send(\n",
    "    topic=topic,\n",
    "    value=message,   \n",
    ").add_callback(on_success).add_errback(on_failure)\n",
    "\n",
    "# No necesitamos esperar confirmaciones, así que simplemente cerramos el productor al final.\n",
    "producer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizando Avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<kafka.producer.future.FutureRecordMetadata at 0x7fb1d1f18370>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecordMetadata(topic='my_new123c', partition=0, topic_partition=TopicPartition(topic='my_new123c', partition=0), offset=0, timestamp=1697853995778, log_start_offset=0, checksum=None, serialized_key_size=-1, serialized_value_size=43, serialized_header_size=-1)\n"
     ]
    }
   ],
   "source": [
    "from confluent_kafka import avro\n",
    "from confluent_kafka.avro import AvroProducer\n",
    "\n",
    "\n",
    "# generamos el schema\n",
    "value_schema_str = \"\"\"\n",
    "{\n",
    "   \"namespace\": \"my.test\",\n",
    "   \"name\": \"value\",\n",
    "   \"type\": \"record\",\n",
    "   \"fields\" : [\n",
    "     {\n",
    "       \"name\" : \"productId\",\n",
    "       \"type\" : \"int\"\n",
    "     },\n",
    "     {\n",
    "       \"name\" : \"product_name\",\n",
    "       \"type\" : \"string\"\n",
    "     }\n",
    "   ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "value_schema = avro.loads(value_schema_str)\n",
    "\n",
    "value = {\"productId\": 1, \"product_name\": \"IPhone\"}\n",
    "\n",
    "avro_producer = AvroProducer(\n",
    "    {\n",
    "        'bootstrap.servers': 'host.docker.internal:9092',\n",
    "        'schema.registry.url': 'http://localhost:8082'  # URL de tu Schema Registry\n",
    "    }, default_value_schema=value_schema\n",
    ")\n",
    "\n",
    "avro_producer.produce(topic='my_new_topic', value=value)\n",
    "avro_producer.flush()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consultando las particiones de un tópico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      " error_code : 0\n",
      " partition : 0\n",
      " leader : 0\n",
      " replicas : [0]\n",
      " isr : [0]\n",
      " offline_replicas : []\n",
      "-----------------------------------------\n",
      " error_code : 0\n",
      " partition : 1\n",
      " leader : 0\n",
      " replicas : [0]\n",
      " isr : [0]\n",
      " offline_replicas : []\n",
      "-----------------------------------------\n",
      " error_code : 0\n",
      " partition : 2\n",
      " leader : 0\n",
      " replicas : [0]\n",
      " isr : [0]\n",
      " offline_replicas : []\n"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaAdminClient\n",
    "\n",
    "admin_client = KafkaAdminClient(bootstrap_servers=\"host.docker.internal:9092\")\n",
    "\n",
    "# Obtiene detalles del tópico\n",
    "topic_metadata = admin_client.describe_topics(topics=[\"my_new_topic\"])[0]\n",
    "\n",
    "partitions = topic_metadata[\"partitions\"] # esto me retorna la lista de particiones que contenga el tópico\n",
    "\n",
    "\n",
    "for partition in partitions:\n",
    "    print(\"-----------------------------------------\")\n",
    "    for key in partition.keys():\n",
    "        print(f\" {key} : {partition[key]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creando particiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CreatePartitionsResponse_v1(throttle_time_ms=0, topic_errors=[(topic='my_new_topic', error_code=0, error_message=None)])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kafka.admin import NewPartitions\n",
    "from kafka import KafkaAdminClient\n",
    "\n",
    "admin_client = KafkaAdminClient(bootstrap_servers=\"host.docker.internal:9092\")\n",
    "\n",
    "\n",
    "new_partitions = NewPartitions(total_count=3)\n",
    "admin_client.create_partitions({\"my_new_topic\": new_partitions})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<kafka.producer.future.FutureRecordMetadata at 0x7fb1d33183d0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecordMetadata(topic='my_new_topic', partition=2, topic_partition=TopicPartition(topic='my_new_topic', partition=2), offset=0, timestamp=1697870379664, log_start_offset=0, checksum=None, serialized_key_size=-1, serialized_value_size=39, serialized_header_size=-1)\n"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaProducer\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers = \"host.docker.internal:9092\",\n",
    "    value_serializer=lambda x:json.dumps(x).encode(\"utf-8\")\n",
    ")\n",
    "\n",
    "message = {\n",
    "    \"productId\": 4,\n",
    "    \"product_name\": \"XYZ\"\n",
    "}\n",
    "\n",
    "\n",
    "producer.send(\n",
    "    value=message,\n",
    "    topic=\"my_new_topic\",\n",
    "    partition=2\n",
    ").add_callback(on_success)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Records can, in addition to key and value, also include headers. Record headers give you the ability to add some metadata about the Kafka record, without adding any extra information to the key/value pair of the record itself. Headers are often used for lineage to indicate the source of the data in the record, and for routing or tracing messages based on header information without having to parse the message itself (perhaps the message is encrypted and the router doesn’t have permissions to access the data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mandando mensaje con headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "\n",
    "\n",
    "# Crear un productor\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=\"host.docker.internal:9092\",\n",
    "    key_serializer=lambda k: json.dumps(k).encode('utf-8'),\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")\n",
    "\n",
    "# Crear encabezados\n",
    "headers = [(\"privacy-level\", b\"YOLO\")]  # Nota: b\"YOLO\" es un byte literal en Python.\n",
    "\n",
    "# Enviar el mensaje\n",
    "producer.send(\"my_new_topic\", key=\"Precision Products\", value=\"France\", headers=headers)\n",
    "\n",
    "# Asegúrate de cerrar el productor cuando hayas terminado.\n",
    "producer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "import json\n",
    "import random\n",
    "import string\n",
    "\n",
    "\n",
    "def random_string(length=10):\n",
    "    \"\"\"Generate a random string of specified length.\"\"\"\n",
    "    return ''.join(random.choice(string.ascii_letters + string.digits) for _ in range(length))\n",
    "\n",
    "def send_thousand_messages(num_messages):\n",
    "\n",
    "    producer = KafkaProducer(\n",
    "        bootstrap_servers = \"host.docker.internal:9092\",\n",
    "        value_serializer = lambda x:json.dumps(x).encode(\"utf-8\")\n",
    "    )\n",
    "\n",
    "    num = range(num_messages)\n",
    "\n",
    "    for i in num:\n",
    "        message  = {\n",
    "            \"ID\":i,\n",
    "            \"name\":random_string(15)\n",
    "        }\n",
    "        producer.send(topic=\"tpc-test-cons\",value=message)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "send_thousand_messages(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class StorageConfig:\n",
    "    azure_account_name:str\n",
    "    azure_account_key :str\n",
    "    bootstrap_servers:str\n",
    "\n",
    "\n",
    "schema = {\n",
    "    \"params\":{\n",
    "        \"bootstrap_props\":{\n",
    "            \"bootstrap_servers\":\"host.docker.internal:9092\",\n",
    "            \"is_required\":{\n",
    "                \"hola\":\"adios\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "StorageConfig(azure_account_key=\"hola\",azure_account_name=\"adios\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config_manager:\n",
    "    def __init__(self) -> None:\n",
    "        self.config = self._get_config()\n",
    "        self._storage_config = None\n",
    "\n",
    "    @property\n",
    "    def storage_config(self):\n",
    "        self._storage_config =  self._get_storage_config()\n",
    "        return self._storage_config\n",
    "\n",
    "\n",
    "    def _get_config(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def _get_storage_config(self):\n",
    "        s = StorageConfig(azure_account_key=\"hola\",azure_account_name=\"adios\")\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StorageProcess:\n",
    "    def __init__(self,config:StorageConfig) -> None:\n",
    "        self.config:StorageConfig = config\n",
    "\n",
    "    def _get_blob_name(self):\n",
    "        pass\n",
    "\n",
    "    def save_kafka(self):\n",
    "        b = self.config.bootstrap_servers  \n",
    "\n",
    "    def save_parquet(self):\n",
    "        pass\n",
    "\n",
    "    def save_csv(self):\n",
    "        pass\n",
    "\n",
    "    def save_local(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    conf_manager = Config_manager()\n",
    "    s = StorageProcess(conf_manager.storage_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
